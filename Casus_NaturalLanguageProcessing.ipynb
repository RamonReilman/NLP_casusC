{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d65ba9",
   "metadata": {},
   "source": [
    "<img src=\"https://bioinf.nl/~davelangers/hanze.png\" align=\"right\" />\n",
    "\n",
    "# <span id=\"0\">Casus *Natural Language Processing*</span>\n",
    "\n",
    "In deze casus maak je kennis met [Natural Language Processing](https://en.wikipedia.org/wiki/Natural_language_processing), het geautomatiseerd verwerken, analyseren en genereren van menselijke taal. Deze [taaltechnologie](https://nl.wikipedia.org/wiki/Taaltechnologie) heeft de laatste jaren met de komst van [Large Language Models](https://en.wikipedia.org/wiki/Large_language_model) een hoge vlucht genomen. We bekijken in deze casus bij wijze van introductie een viertal aspecten. Er zit wel een opbouw in de onderwerpen, maar je kunt sommige delen van latere opdrachten oplossen zonder de eerdere opdrachten helemaal af te hebben.\n",
    "\n",
    "* **<a href=\"#Les-1:-Tokenisatie\">Les 1: Tokenisatie</a>**\n",
    "\n",
    "* **<a href=\"#Les-2:-Bag-Of-Words classificatie\">Les 2: Bag-Of-Words classificatie</a>**\n",
    "\n",
    "* **<a href=\"#Les-3:-N-gram-tekstgeneratie\">Les 3: N-gram tekstgeneratie</a>**\n",
    "\n",
    "* **<a href=\"#Les-4:-Embeddings\">Les 4: Embeddings</a>**\n",
    "\n",
    "Als je uitgebreider aan de slag wil met bv. [Generative Pre-trained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) (GPTs) dan zou je dat kunnen doen in je verdiepend project.\n",
    "\n",
    "<a href=\"https://navigate360.com/blog/what-is-natural-language-processing/\"><img src=\"https://navigate360.com/wp-content/uploads/2021/03/NLP-image.png\" width=\"50%\" /></a>\n",
    " \n",
    "<div class=\"alert alert-warning\">\n",
    "    \n",
    "# ⚠️\n",
    "\n",
    "**Opmerking:** Implementeer de functionaliteit in deze casus die gevraagd wordt onder de \"Opdracht\" kopjes (d.w.z. de python scripts en module) zelf gebruikmakend van alle [standaardmodules](https://docs.python.org/3/library/index.html) van python, maar maak geen wrappers rond bestaande externe modules, tenzij expliciet anders wordt vermeld; voor de \"Toepassing\" onderdelen (d.w.z. de notebooks) ben je vrij om te gebruiken wat je wil. Houd je werk bij in een (gezamenlijke) repository die je na afloop inlevert via BrightSpace.\n",
    "\n",
    "</div>\n",
    "\n",
    "Deze casus is mede geïnspireerd op en deels afgeleid van het werk van [Bart Barnard](https://www.bartbarnard.nl) dat in studiejaar 2024-2025 behandeld werd als [Casus B](https://bioinf.nl/~bbarnard/thema10/casusb/inleiding.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e595b90-f738-4406-9495-1951c8d4846e",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Les 1: Tokenisatie\n",
    "\n",
    "### Inleiding\n",
    "\n",
    "Tokenisatie is het proces waarin een tekst wordt omgezet in een opeenvolgende reeks *tokens* die kunnen worden geidentificeerd met een numeriek label (d.w.z. een integer).\n",
    "\n",
    "Bijvoorbeeld, de tekst:\n",
    "\n",
    "<span style=\"font-family: monospace\">Dit is de derde casus in een module Modelleren van Kanker van de opleiding bio-informatica aan de Hanzehogeschool Groningen die gaat over Natural Language Processing.</span>\n",
    "\n",
    "zou als volgt kunnen worden verdeeld in tokens:\n",
    "\n",
    "<span style=\"font-family: monospace\"><span style=\"color: red\">Dit</span><span style=\"color: green\"> is</span><span style=\"color: blue\"> de</span><span style=\"color: orange\"> derde</span><span style=\"color: lightgrey\"> cas</span><span style=\"color: red\">us</span><span style=\"color: green\"> in</span><span style=\"color: blue\"> een</span><span style=\"color: orange\"> module</span><span style=\"color: lightgrey\"> Modell</span><span style=\"color: red\">eren</span><span style=\"color: green\"> van</span><span style=\"color: blue\"> K</span><span style=\"color: orange\">anker</span><span style=\"color: lightgrey\"> van</span><span style=\"color: red\"> de</span><span style=\"color: green\"> opleiding</span><span style=\"color: blue\"> bio</span><span style=\"color: orange\">-in</span><span style=\"color: lightgrey\">format</span><span style=\"color: red\">ica</span><span style=\"color: green\"> aan</span><span style=\"color: blue\"> de</span><span style=\"color: orange\"> Han</span><span style=\"color: lightgrey\">ze</span><span style=\"color: red\">hog</span><span style=\"color: green\">eschool</span><span style=\"color: blue\"> Groningen</span><span style=\"color: orange\"> die</span><span style=\"color: lightgrey\"> gaat</span><span style=\"color: red\"> over</span><span style=\"color: green\"> Natural</span><span style=\"color: blue\"> Language</span><span style=\"color: orange\"> Processing</span><span style=\"color: lightgrey\">.</span></span>\n",
    "\n",
    "die vervolgens overeenkomen met de volgende getalwaarden:\n",
    "\n",
    "<span style=\"font-family: monospace\">33946, 382, 334, 60289, 2108, 385, 306, 1351, 8827, 79638, 4229, 1164, 658, 67579, 1164, 334, 69721, 18885, 4200, 4078, 1578, 3136, 334, 21513, 1547, 96219, 68507, 99393, 1076, 13896, 1072, 23735, 20333, 44532, 558</span>\n",
    "\n",
    "N.B.: bovenstaand is de tokenisatie zoals die wordt uitgevoerd door het [GPT-4o model](https://platform.openai.com/tokenizer). Veelvoorkomende woorden zijn meestal een eigen token, maar sommige zeldzame woorden worden gerepresenteerd door een aantal kleinere tokens.\n",
    "\n",
    "Een vorm van tokenisatie die tegenwoordig veel gebruikt wordt is [byte-pair encoding](https://en.wikipedia.org/wiki/Byte-pair_encoding). Hierbij wordt een tekst aanvankelijk opgesplitst in tokens die uit losse lettertekens bestaan. Daarna wordt herhaaldelijk bepaald welke twee opeenvolgende tokens het vaakst na elkaar voorkomen, en voor deze combinatie van twee tokens wordt één nieuw extra token gedefinieerd. Meestal worden tokens die interpunctie coderen (spaties, komma's, etc.) hiervan uitgesloten. Deze procedure wordt herhaald tot een vooraf aangegeven aantal verschillende tokens is bereikt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd8051d-d902-4ede-abf3-158ba7a60975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/HEikzVL-lZU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/HEikzVL-lZU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ba4a7-5396-4dfb-8d69-fcb570fbcffc",
   "metadata": {},
   "source": [
    "### Opdracht\n",
    "\n",
    "Schrijf een python-script `tokenize` dat kan worden aangeroepen vanaf de command line en dat drie verschillende functionaliteiten implementeert:\n",
    "\n",
    "1. Het kan een gegeven leesbaar `*.txt` tekstbestand inlezen en hieruit door middel van byte-pair encoding een tokenisatie afleiden. Deze gemaakte byte-pair encoding wordt weggeschreven naar een nieuw `*.enc` databestand (d.w.z. de correspondentie tussen tekst en tokens, niet de omzetting van deze tekst zelf).\n",
    "2. Het kan een gegeven leesbaar `*.txt` tekstbestand inlezen en hierop een gemaakte byte-pair encoding uit een `*.enc` databestand toepassen om dit om te zetten in tokens, en het resultaat vervolgens wegschrijven als een `*.tok` datafile met tokens.\n",
    "3. Het kan een gegeven `*.tok` bestand met tokens inlezen en hierop een gemaakte byte-pair encoding uit een `*.enc` bestand toepassen om dit terug om te zetten in tekst, en het resultaat vervolgens wegschrijven als een leesbaar `*.txt` tekstbestand.\n",
    "\n",
    "Kies zelf geschikte bestandsindelingen voor de bestandstypen met encodings (`*.enc`) en met tokens (`*.tok`). De gebruiker dient te kunnen aangeven hoeveel verschillende tokens er gemaakt mogen worden en/of hoe vaak een paar tokens ten minste moet voorkomen om te worden samengevoegd tot een nieuw token. Bedenk ook zelf hoe je omgaat met interpunctie en andere niet-alfanumerieke symbolen, of met verschillen in hoofdlettergebruik (of geef de gebruiker de mogelijkheid om hier keuzes in te maken). Je mag ook de mogelijkheid geven functionaliteit te combineren (bv. een byte-pair encoding bepalen uit een tekstdocument en dit tekstdocument gelijktijdig omzetten naar tokens in één aanroep).\n",
    "\n",
    "Geef bondige documentatie van het gebruik weer als de gebruiker je script aanroept met `tokenize --help` of `tokenize -h`.\n",
    "\n",
    "Breng klassen en functies die van algemeen belang zijn onder in een aparte module `nlp.py` die gedeeld is over alle lessen, en importeer deze in je script. Je script kan daarnaast eigen klassen en functies definiëren voor eigen gebruik.\n",
    "\n",
    "### Toepassing\n",
    "\n",
    "Copy/paste de tekst van de [Nederlandstalige](https://nl.wikipedia.org/wiki/Kanker) en [Engelstalige](https://en.wikipedia.org/wiki/Cancer) Wikipedia pagina's over kanker/cancer als tekst zonder opmaak (of scrape deze automatisch met bv. [beautifulsoup](https://www.tutorialspoint.com/beautiful_soup/beautiful_soup_get_text_method.htm)). Bepaal op basis van beide bestanden byte-pair encodings met evenveel verschillende tokens. Pas beide encodings toe op allebei de teksten (je krijgt dus vier bestanden met tokens!).\n",
    "\n",
    "Kun je iets zeggen over het aantal tokens waaruit deze teksten bestaan? Kun je de verschillen verklaren?\n",
    "\n",
    "Documenteer deze stappen in de vorm van een kort Jupyter notebook genaamd `les1.ipynb`. Je kunt hierin naast specieke code en tekst de functionaliteit uit je `nlp.py` module importeren en/of je script rechtstreeks aanroepen vanuit een code cell met de `%%bash` cell magic. Gebruik bondige tekst om de stappen toe te lichten en om de uitkomsten waar nodig kritisch van een interpretatie te voorzien.\n",
    "\n",
    "Als de tijd het toelaat, vergelijk dan het resultaat van je eigen tokenizer met dat van de [byte_pair_tokenizer](https://keras.io/keras_hub/api/tokenizers/byte_pair_tokenizer/) van `keras_hub`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fdc664-4db5-495f-a79c-810a1646598b",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Les 2: Bag-Of-Words classificatie\n",
    "\n",
    "### Inleiding\n",
    "\n",
    "Een [bag-of-words model](https://en.wikipedia.org/wiki/Bag-of-words_model) beschouwt een document als een ongeordende hoeveelheid tokens (zeg, woorden). Het houdt wel rekening met hoeveel tokens er zijn en hoe vaak die voorkomen, maar niet met de volgorde van die tokens. Typisch wordt elk document gerepresenteerd als een reeks getallen, met voor elk mogelijk token één getalwaarde.\n",
    "\n",
    "1. Een *multi-hot* encoding (soms ook onterecht one-hot encoding genoemd) codeert welke tokens voorkomen in een document. Het resultaat is een reeks 0-en en 1-en voor tokens die respectievelijk niet of wel voorkomen in een document.\n",
    "2. Een *frequency* encoding codeert voor elk voorkomend token de frequentie (als aantal, als fractie of als percentage). Het resultaat is een reeks integers of getallen van 0.0 tot 1.0 of 0 tot 100.\n",
    "3. En *Term-Frequency Inverse-Document-Frequency* (TF-IDF) encoding telt voor elk token hoe vaak dit voorkomt in elk afzonderlijk document en berekent dan een getalwaarde volgens onderstaande formule; dit kent een hoog belang toe aan tokens die veel voorkomen in een document, maar nauwelijks in andere documenten te vinden zijn.\n",
    "\n",
    "$$\n",
    "\\text{TF-IDF}_{t,d} = \\frac{n_{t,d}}{n_d} \\cdot \\ln \\left( \\frac{N}{N_t} \\right)\n",
    "$$\n",
    "\n",
    "Hierin is $n_{t,d}$ het aantal keren dat token $t$ voorkomt in document $d$ en $n_d$ het totale aantal tokens in document $d$ (dus de breuk $\\frac{n_{t,d}}{n_d}$ geeft de fractie van tokens in het document $d$ die gelijk zijn aan token $t$); $N_t$ is het aantal documenten dat token $t$ bevat en $N$ het totale aantal documenten (dus de breuk $\\frac{N_t}{N}$ geeft de fractie documenten die token $t$ bevatten)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e73f4b9b-418b-4591-9573-1ba3f8554c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/irzVuSO8o4g\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/irzVuSO8o4g\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e51ccb6-59ad-49c5-a0c4-f73c33d37647",
   "metadata": {},
   "source": [
    "### Opdracht\n",
    "\n",
    "Stel dat een leesbaar `*.txt` tekstbestand gegeven is met daarin een corpus bestaande uit een reeks documenten. Elk document staat op één regel (d.w.z. een document kan uit meerdere zinnen bestaan, maar bevat geen regeleinden). Schrijf een python-script `bagofwords` dat kan worden aangeroepen vanaf de command line en dat elk document omzet in een reeks getalwaarden. De gebruiker dient te kunnen aangeven of een multi-hot, telling/frequentie, of TF-IDF encoding gebruikt dient te worden. Het resultaat dient te worden opgeslagen in een `*.bow` databestand met de bag-of-words representaties. Kies zelf een geschikte bestandsindeling.\n",
    "\n",
    "Het dient ook mogelijk te zijn om meerdere leesbare `*.txt` tekstbestanden mee te geven. Die dienen dan allemaal *gezamenlijk* te worden omgezet in hun eigen `*.bow` databestanden. Daarbij dient voor elk bestand dezelfde tokenisatie gebruikt te worden en ook de TF-IDF berekening dient gebaseerd te worden op alle documenten tezamen (d.w.z. maak één byte-pair encoding op basis van alle informatie in alle documenten gezamenlijk, en tel de frequenties van documenten die een zeker token bevatten over alle documenten heen).\n",
    "\n",
    "Geef bondige documentatie van het gebruik weer als de gebruiker je script aanroept met `bagofwords --help` of `bagofwords -h`.\n",
    "\n",
    "Breng klassen en functies die van algemeen belang zijn onder in een aparte module `nlp.py` die gedeeld is over alle lessen, en importeer deze in je script. Je script kan daarnaast eigen klassen en functies definiëren voor eigen gebruik.\n",
    "\n",
    "### Toepassing\n",
    "\n",
    "Download een set PubMed abstracts met een aantal zoektermen, enerzijds een corpus op het gebied van kanker en anderzijds een corpus zonder enige relatie hiermee. Sla voor elk document de titel plus abstract op als één tekstregel. Je krijgt nu twee bestanden: een met inhoud op het gebied van kanker en een met ongerelateerde inhoud. Zorg dat het aantal abstracts in beide bestanden niet al te extreem uiteenloopt. Selecteer een stevige verzameling, maar houd het wel behapbaar (je hoeft bv. niet abstracts van alle jaren te selecteren). Voel je vrij om eventueel meer dan twee bestanden te genereren (bv. verschillende soorten kanker, of uiteenlopende niet-kanker gerelateerde onderwerpen), en produceer verschillende encodings (multi-hot, telling/frequency, TF-IDF). Zet de corpora tenslotte om in een bag-of-words encoding middels je script.\n",
    "\n",
    "Je kan PubMed data downloaden via een [API](https://www.ncbi.nlm.nih.gov/books/NBK25500/), maar hieraan zitten beperkingen qua hoeveelheden dataverkeer. Echter, in de BIN-netwerkfolder `/commons/data/NCBI/PubMed` vind je bestanden met de inhoud van PubMed, gepickled als geneste dictionaries. Met onderstaande voorbeeldcode kun je bijvoorbeeld de titel plus abstract onttrekken.\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "with open('/commons/data/NCBI/PubMed/pubmed21n????.xml.dump', 'rb') as pickle_file:\n",
    "    publications = pickle.load(pickle_file)['PubmedArticleSet']['PubmedArticle']\n",
    "def get_text(index):\n",
    "    publication = publications[index]['MedlineCitation']['Article']\n",
    "    title = publication['ArticleTitle']\n",
    "    if isinstance(title, dict):\n",
    "        title = title['#text']\n",
    "    abstract = publication['Abstract']['AbstractText']\n",
    "    if isinstance(abstract, dict):\n",
    "        abstract = abstract['#text']\n",
    "    return title + ' ' + abstract\n",
    "print(get_text(0))\n",
    "```\n",
    "\n",
    "Vervang de `????` in de bestandsnaam door een van de vele beschikbare bestandsnummers. Merk op dat niet alle publicaties een abstract hebben; je zult dan een `KeyError` krijgen. Bekijk zonodig de structuur van de `publications` variabele nader; deze bevat bijvoorbeeld ook vaak het journal en een sectie met keywords die informatie kunnen geven over het onderwerp van de publicatie.\n",
    "\n",
    "Importeer vervolgens een Machine Learning model uit `sklearn`. Kies bijvoorbeeld een Naive Bayes classifier die van oudsher gebruikt werd om spam vs. niet-spam e-mail teksten te onderscheiden (maar ook andere classifiers zijn prima). Importeer de data uit de bag-of-words bestanden en label alle documenten met een klasselabel (bv. 0 voor niet-kanker en 1 voor kanker, of meerdere labels als je meerdere klassen hebt). Hussel ze en splits ze in twee datasets: eentje om te trainen en eentje om te testen. Train je classifier met de trainingsdata en bepaal daarna de nauwkeurigheid die behaald wordt op de testdata.\n",
    "\n",
    "Maakt het uit welk soort bag-of-words encoding je gebruikt? Komen de zoektermen die je gebruikt hebt om publicaties te zoeken voor in de tokens, en zo ja, hebben die een grote invloed op het eindresultaat?\n",
    "\n",
    "Documenteer deze stappen in de vorm van een kort Jupyter notebook genaamd `les2.ipynb`. Je kunt hierin naast specieke code en tekst de functionaliteit uit je `nlp.py` module importeren en/of je script rechtstreeks aanroepen vanuit een code cell met de `%%bash` cell magic. Gebruik bondige tekst om de stappen toe te lichten en om de uitkomsten waar nodig kritisch van een interpretatie te voorzien.\n",
    "\n",
    "Als de tijd het toelaat, vergelijk dan het resultaat van je eigen bag-of-words model met dat van de [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) en/of [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) van `sklearn`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca922b9-3b4c-4560-993d-2eaf345a3263",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Les 3: N-gram tekstgeneratie\n",
    "\n",
    "### Inleiding\n",
    "\n",
    "Een $n$-gram is een opeenvolgende reeks van $n$ tokens (bv. letters, lettergrepen, of woorden) die worden verzameld uit een hoeveelheid tekst (of soms ook andere sequenties, zoals nucleotide- of aminozuursequenties). Vergeleken met individuele tokens bevatten $n$-grams ook informatie over volgorde.\n",
    "\n",
    "Bijvoorbeeld, de tekst:\n",
    "\n",
    "```\n",
    "Dit is de derde casus in een module Modelleren van Kanker van de opleiding bio-informatica aan de Hanzehogeschool Groningen die gaat over Natural Language Processing.\n",
    "```\n",
    "\n",
    "kan worden omgezet in de volgende reeks van — onderling overlappende — 3-grams:\n",
    "\n",
    "```\n",
    "Dit is de\n",
    "is de derde\n",
    "de derde casus\n",
    "derde casus in\n",
    "casus in een\n",
    "in een module\n",
    "een module Modelleren\n",
    "module Modelleren van\n",
    "Modelleren van Kanker\n",
    "van Kanker van\n",
    "Kanker van de\n",
    "van de opleiding\n",
    "de opleiding bio-informatica\n",
    "opleiding bio-informatica aan\n",
    "bio-informatica aan de\n",
    "aan de Hanzehogeschool\n",
    "de Hanzehogeschool Groningen\n",
    "Hanzehogeschool Groningen die\n",
    "Groningen die gaat\n",
    "die gaat over\n",
    "gaat over Natural\n",
    "over Natural Language\n",
    "Natural Language Processing\n",
    "```\n",
    "\n",
    "We gebruikten hier tokens die bestaan uit een heel woord en negeren overige interpunctie.\n",
    "\n",
    "Een [$n$-gram language model](https://en.wikipedia.org/wiki/Word_n-gram_language_model) is een statistisch model dat de kans op een zeker volgend token berekent, gegeven een zeker aantal voorgaande tokens. Het doet dit in essentie door in de verzameling van alle mogelijke $n$-grams in een trainingstekst te tellen hoe vaak elk laatste token voorkomt na $n-1$ vorige tokens en daar voorwaardelijke kansen uit af te leiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d458255-41fa-435c-875d-97a70bd385f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Vc2C1NZkH0E\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/Vc2C1NZkH0E\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c1a73c-9d4d-432e-816b-30fcc11f5267",
   "metadata": {},
   "source": [
    "### Opdracht\n",
    "\n",
    "Schrijf een python-script `ngram` dat kan worden aangeroepen vanaf de command line. Het script dient een `*.tok` databestand met tokens te kunnen ontvangen als argument (optioneel, maak het mogelijk meerdere bestanden te ontvangen). Train hiermee een $n$-gram language model voor een door de gebruiker gegeven $n$ door te inventariseren welke $n-1$ tokens gevolgd worden door welke laatste tokens, en met welke kans. Gebruik deze vervolgens om, startend vanaf een willekeurig begin, steeds nieuwe tokens te trekken volgens de berekende waarschijnlijkheid. (Merk op dat dit lijkt op de `sample` methode van het Hidden Markov Model uit Casus A!) Zoek zelf een oplossing voor situaties waar je bijvoorbeeld $n-1$ tokens moet aanvullen die nooit in die combinatie in de trainingsdata zijn voorgekomen. Je script dient uiteindelijk een reeks tokens van een gevraagde lengte weg te schrijven naar een nieuw `*.tok` databestand.\n",
    "\n",
    "Geef bondige documentatie van het gebruik weer als de gebruiker je script aanroept met `ngram --help` of `ngram -h`.\n",
    "\n",
    "Breng klassen en functies die van algemeen belang zijn onder in een aparte module `nlp.py` die gedeeld is over alle lessen, en importeer deze in je script. Je script kan daarnaast eigen klassen en functies definiëren voor eigen gebruik.\n",
    "\n",
    "### Toepassing\n",
    "\n",
    "Op [Project Gutenberg](https://www.gutenberg.org/) zijn open-source teksten te vinden van vele literaire werken. Je vindt er ook de publicatie \"Cancer: Its Cause and Treatment\" volume [I](https://www.gutenberg.org/cache/epub/59550/pg59550.txt) & [II](https://www.gutenberg.org/cache/epub/59312/pg59312.txt) van Lucius Duncan Bulkley; zijn ideeën over de oorzaak van kanker waren nogal omstreden en zijn inmiddels achterhaald, maar dit boek was een eeuw geleden zeer invloedrijk. Download deze tekstdocumenten en plak de teksten van beide volumes aan elkaar, voer er tokenisatie op uit, en bepaal hieruit de frequenties van 1-grams, 2-grams, en 3-grams. Gebruik deze om willekeurige fragmenten \"Engelse tekst\" te genereren.\n",
    "\n",
    "Leveren langere $n$-grams \"geloofwaardiger Engels\" op dan kortere $n$-grams? Hoe hangen de resultaten af van het aantal tokens dat je toestaat tijdens de tokenisatie?\n",
    "\n",
    "Documenteer deze stappen in de vorm van een kort Jupyter notebook genaamd `les3.ipynb`. Je kunt hierin naast specieke code en tekst de functionaliteit uit je `nlp.py` module importeren en/of je script rechtstreeks aanroepen vanuit een code cell met de `%%bash` cell magic. Gebruik bondige tekst om de stappen toe te lichten en om de uitkomsten waar nodig kritisch van een interpretatie te voorzien.\n",
    "\n",
    "Als de tijd het toelaat, vergelijk dan het resultaat van je eigen $n$-gram model met dat van de [LanguageModel.generate](https://github.com/shayan09/ngam-text-generation) functie van de `nltk.lm.MLE` klasse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8dcc3f-d4ed-4b1c-86dc-c621f7dae4ac",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Les 4: Embeddings\n",
    "\n",
    "### Inleiding\n",
    "\n",
    "Embeddings maken gebruik van één van twee technieken: de Continuous Bag of Words (CBOW) of de skip-gram. Feitelijk zijn deze twee algoritmen tegengesteld aan elkaar: CBOW probeert op basis van de context het missende woord te vinden, terwijl skip-gram op basis van het missende woord de context probeert te achterhalen. We richten ons hier op CBOW, dat werkt door met een bepaald window over het corpus van zinnen te gaan en dan telkens $n$ tokens links en $n$ tokens rechts van het middelste token als input te nemen en het woord zelf als output. Bijvoorbeeld, voor $n=2$ voorspelt CBOW dat het meest waarschijnlijke woord in de context \"de student ... een broodje\" vermoedelijk \"eet\" is, maar andere woorden zoals \"verorbert\", \"bestelt\" of \"smeert\" passen hier ook. (We gebruiken in dit voorbeeld hele woorden als tokens.)\n",
    "\n",
    "<a href=\"https://www.researchgate.net/publication/326588219_Extending_Thesauri_Using_Word_Embeddings_and_the_Intersection_Method\" /><img src=\"https://www.researchgate.net/profile/Daniel-Braun-6/publication/326588219/figure/fig1/AS:652185784295425@1532504616288/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations.png\" width=\"50%\" /></a>\n",
    "\n",
    "Het doel is om een model te trainen om het juiste ontbrekende token te voorspellen gegeven een bepaalde set context-tokens. Er zijn vele manieren om dit te implementeren, waarvan sommige behoorlijk complex zijn en veel trainingsdata vereisen. We beperken ons hier tot een vereenvoudigde variant waarbij we de input van het model coderen als een bag-of-words multi-hot encoding. De output van het model is een token.\n",
    "\n",
    "We kunnen een voorspelling produceren met een ondiep neuraal netwerk model. Een dergelijk model heeft één verborgen laag met een aantal neuronen $N$ dat we zelf kunnen kiezen: bijvoorbeeld, het model [Llama 3](https://en.wikipedia.org/wiki/Llama_(language_model)#Llama_3) gebruikt er 4096, maar wij zullen het houden bij een veel kleiner aantal. Elk neuron produceert een eigen getal dat een gewogen gemiddelde is van alle getallen in de invoerlaag. De neuronen in de verborgen laag \"leren\" geschikte gewichten om de lange maar sparse context-vector om te zetten in een korte maar dense \"embedding vector\". Dit is een reeks getallen die op een of andere manier de relevante informatie uit de context codeert.\n",
    "\n",
    "Vervolgens is er een uitvoerlaag die de kans dient te leren op elk ontbrekend token uit de informatie in de embedding vector. Het idee is nu dat de embedding zo geleerd wordt dat de relevante semantische informatie gecodeerd wordt. Omdat de informatie veel compacter is dan de input zal het model ongetwijfeld fouten maken, maar de hoop is dat dit voornamelijk fouten zijn die ook plausibel zijn. Bijvoorbeeld, in bovenstaande zin zouden de voorspellingen \"verorbert\", \"bestelt\" of \"smeert\" weliswaar niet gelijk zijn aan \"eet\", maar ook een acceptabele zin vormen. Het is dus waarschijnlijk dat in de uitvoerlaag de woorden \"verorbert\", \"bestelt\" of \"smeert\" soortgelijke verbindingen hebben met de embedding layer als het woord \"eet\". De sterkten van de verbindingen in de uitvoerlaag voor elk woord coderen daarmee als het ware de \"betekenis\" van dat woord.\n",
    "\n",
    "Het doel van dit model is in de praktijk niet zozeer om daadwerkelijk ontbrekende tokens te voorspellen. Interessanter is hoe de embedding relateert aan de voorspelling. De gewichten in de uitvoerlaag zeggen iets over de betekenis van een individueel token en kunnen dus gebruikt worden om middels een beperkt aantal getalwaarden de meest relevante betekeniskenmerken van dat token samen te vatten. Large Language Modellen maken intern gebruik van dit soort representaties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27d2a7af-723c-4277-b16b-c818294a32b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/MYjQV5jwqCY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<iframe width=\"640\" height=\"360\" src=\"https://www.youtube.com/embed/MYjQV5jwqCY\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c213be-8f7f-42f5-88a8-3dc93d325a02",
   "metadata": {},
   "source": [
    "### Opdracht\n",
    "\n",
    "Schrijf een python-script `embed` dat kan worden aangeroepen vanaf de command line. Het script dient een `*.tok` databestand met tokens te kunnen ontvangen als argument. Train hiermee een embedding model. De gebruiker zou de groote van de windows (in de vorm van $n$) op moeten kunnen geven, evenals het aantal hidden neurons. Schrijf de embedding die elk token associeert met een vector getallen weg naar een nieuw `*.emb` databestand (hiervoor is mogelijk ook het `*.enc` databestand met de byte-pair encoding nodig). Kies weer zelf een geschikt bestandsformaat.\n",
    "\n",
    "Dit is een conceptueel uitdagende opdracht! In [dit vervolg](https://www.youtube.com/embed/39w4WSxvRQM) op de video hierboven wordt echter een vergelijkbaar modelletje gebouwd in `python`. Er zijn wel een aantal voorname verschillen:\n",
    "\n",
    "* De gegeven implementatie werkt met woorden en verwijdert stopwoorden; wij gebruiken tokens.\n",
    "* De code in de video gebruikt 2-grams; wij zullen het middelste woord in een ($2n+1$)-gram voorspellen op basis van de $n$ eerdere en $n$ volgende woorden\n",
    "* De video maakt gebruik van een one-hot encoding voor zowel de input als de output; wij gebruiken een multi-hot encoding voor de input en voorspellen rechtstreeks het token voor de output.\n",
    "* Het model in de video heeft slechts 2 hidden neurons; wij kiezen er gewoonlijk meer.\n",
    "* De video gebruikt `keras`; wij gaan hier de [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) van `sklearn` gebruiken (wees alert omtrent de eigenschappen van de verborgen laag die je dient op te geven).\n",
    "\n",
    "De getrainde MLPClassifier slaat de getrainde waarden op in een attribuut `coef_`: de eerste waarde in deze lijst bevat de gewichten tussen de invoerlaag en de verborgen laag, terwijl de tweede waarde de gewichten bevat tussen de verborgen laag en de uitvoerlaag. Hieruit haal je voor elk token de embedding vector.\n",
    "\n",
    "Geef bondige documentatie van het gebruik weer als de gebruiker je script aanroept met `embed --help` of `embed -h`.\n",
    "\n",
    "Breng klassen en functies die van algemeen belang zijn onder in een aparte module `nlp.py` die gedeeld is over alle lessen, en importeer deze in je script. Je script kan daarnaast eigen klassen en functies definiëren voor eigen gebruik.\n",
    "\n",
    "### Toepassing\n",
    "\n",
    "Voeg de Engelstalige wikipedia-pagina, de verzameling titels en abstracts over kanker, en de publicatie van Bulkley samen tot één groot leesbaar `*.txt` document. Voer er tokenisatie op uit, leid er een embedding uit af. Speel zelf met de diverse instellingen die de verschillende functies hebben. Maak net als in de video een grafische visualisatie van de ligging van de tokens. Als je meer dan 2 neuronen in de verborgen laag hebt gekozen en de embedding vectoren meer dan 2 dimensies hebben, gebruik dan PCA om dit te reduceren tot 2 dimensies.\n",
    "\n",
    "Als je voor elk token een ander token zoekt dat er het dichtste bij ligt in de embeddingsruimte, vind je dan paren tokens die verbanden lijken te hebben?\n",
    "\n",
    "Documenteer deze stappen in de vorm van een kort Jupyter notebook genaamd `les4.ipynb`. Je kunt hierin naast specieke code en tekst de functionaliteit uit je `nlp.py` module importeren en/of je script rechtstreeks aanroepen vanuit een code cell met de `%%bash` cell magic. Gebruik bondige tekst om de stappen toe te lichten en om de uitkomsten waar nodig kritisch van een interpretatie te voorzien.\n",
    "\n",
    "Als de tijd het toelaat, proberen dan je getrainde model te visualiseren in de online [embedding layer projector](http://projector.tensorflow.org/) (zie aldaar onder de *Load* knop hoe je je embedding dient aan te leveren als tab-delimited bestanden)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a939ddc",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "&copy; 2025 - Dave R.M. Langers <d.r.m.langers@pl.hanze.nl>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
