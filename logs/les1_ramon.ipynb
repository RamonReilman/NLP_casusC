{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Les 1 Ramon Reilman\n",
    "In deze notebook ga ik mijn script `./scripts/tokenize_Ramon.py` toepassen op 2 wikipedia pagina's. Deze 2 (Cancer, Kanker) kan ik inladen door gebruik  te maken van `wikipediaapi` dit is een library die het scrapen van wikipedia pagina's gemakkelijk maakt."
   ],
   "id": "6f246ae28f9c2453"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T12:46:22.996624Z",
     "start_time": "2025-12-09T12:46:22.094817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import wikipediaapi\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(user_agent='your-user-agent',language=\"en\")\n",
    "englishcancer_text = wiki_wiki.page(\"Cancer\").text\n",
    "\n",
    "dutchcancer_text = wikipediaapi.Wikipedia(user_agent='your-user-agent',language=\"nl\").page(\"Kanker\").text"
   ],
   "id": "6055ade9accf1fe7",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Na het inlezen van deze 2 verschillende pagina's maak ik een functie die het wegschrijven van deze data gemakkelijk maakt.",
   "id": "4f289a50e70eefbd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T12:46:23.032885Z",
     "start_time": "2025-12-09T12:46:23.023246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def write_text(text, output):\n",
    "    try:\n",
    "        with open(output, \"w\") as f:\n",
    "            f.write(text)\n",
    "    except IOError as _:\n",
    "        print(f\"IO error for path: {output}\")\n",
    "        return\n"
   ],
   "id": "df0681e6eb9cd281",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Omdat het tokenizing scriptje bestanden als input neemt moeten de 2 teksten weggeschreven worden naar 2 .txt bestanden, dat wordt in de for-loop hieronder gedaan.",
   "id": "4f1bae7f1ba169ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T12:46:23.102367Z",
     "start_time": "2025-12-09T12:46:23.092316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "outputs = [\"cancer_nl.txt\", \"cancer_en.txt\"]\n",
    "texts = [dutchcancer_text, englishcancer_text]\n",
    "\n",
    "for text, output in zip(texts, outputs):\n",
    "    write_text(text, output)"
   ],
   "id": "e10d1d30238b945b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Generating the .enc files",
   "id": "3a393fbed26953bc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T12:46:23.180737Z",
     "start_time": "2025-12-09T12:46:23.157096Z"
    }
   },
   "cell_type": "code",
   "source": "from scripts import tokenize_Ramon",
   "id": "9e579b87822fbff0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T12:46:43.866066Z",
     "start_time": "2025-12-09T12:46:23.221721Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for output in outputs:\n",
    "    args_enc = [\"generate-enc\", \"--txt_file\", output, \"-o\", f\"{output}.enc\", \"-m\", \"1000\"]\n",
    "    tokenize_Ramon.main(args_enc)\n",
    "\n",
    "for output in outputs:\n",
    "    args_tok = [\"generate-toc\", \"--txt_file\", output, \"--enc_file\", f\"{outputs[0]}.enc\", \"-o\", f\"{output}_nl_enc.tok\"]\n",
    "    args_tok2 = [\"generate-toc\", \"--txt_file\", output, \"--enc_file\", f\"{outputs[1]}.enc\", \"-o\", f\"{output}_en_enc.tok\"]\n",
    "    tokenize_Ramon.main(args_tok)\n",
    "    tokenize_Ramon.main(args_tok2)\n",
    "\n"
   ],
   "id": "46a032af7f7da4e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nu zijn er 2 enc bestandjes gemaakt **en** 4.tok bestandjes. Voor elk input bestand (engels en nederlands) zijn er tokens gegenereerd voor zowel de engelse als de nederlandse encoding om het verschil hiertussen te bekijken. Voordat ik deze ga openen en bekijken wil ik eerst een verwachting uitbrengen. Omdat de 2 talen, vooral bij langere woorden, niet overeenkomen verwacht ik dat de tokens gemaakt met de andere taal veel korter zullen zijn en voornamelijk bestaan uit letters, omdat de tokens niet voorkomen in de andere taal.",
   "id": "7146ad95ed936264"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T14:52:37.394368Z",
     "start_time": "2025-12-10T14:52:37.370285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "\n",
    "head \"../output.tok\""
   ],
   "id": "d8b6258571d24e5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The_future_king_is_the_prince\n",
      "Daughter_is_the_princess\n",
      "Son_is_the_prince\n",
      "Only_a_man_can_be_a_king\n",
      "Only_a_woman_can_be_a_queen\n",
      "The_princess_will_be_a_queen\n",
      "The_prince_is_a_strong_man\n",
      "The_princess_is_a_beautiful_woman\n",
      "Prince_is_only_a_boy_now\n",
      "Prince_will_be_king\n"
     ]
    }
   ],
   "execution_count": 5
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
