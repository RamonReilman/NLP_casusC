{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "markdown",
   "source": "# Embeddings",
   "id": "50e94055548d6c69"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Word embedding = representatie waarbij elk token wordt gerepresenteerd door een vector (lijst van floats).\n",
    "v = [1,2,3]\n",
    "w = [4,5,6]\n",
    "v+w = [5,7,9]\n",
    "2 * v = [2,4,6]\n",
    "\n",
    "tokens worden in een n-dimensionale ruimte geplaatst, en zo kan er correlatie tokens gemaakt worden. tokens die opelkaar lijken in dezelfde hoek. tokens met relatie hebben een zelfde afstand / richting.\n",
    "Hoe maak je dit? Betekenis van tokens kan je vinden in de context:\n",
    "n-grams."
   ],
   "id": "5c893e8bc3006e09"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\"de student eet een broodje\" allemaal een token, 5-gram. Train model om context te leren.\n",
    "\n",
    "\n",
    "\"de student ... een broodje\"\n",
    "\n",
    "\"eet\" \"verobert\" \"koopt\" \"wil\" Eet en Verorbert liggen bij elkaar in de buurt. Dit kan bepaald worden via een neuraal netwerk.\n",
    "De context gaan in zo'n NN (multi-hot) (behalve het ontbrekende token) output: elk token een eigen neuron -> p (token). Dit nn maakt gebruik van een hidden layer.\n",
    "\n",
    "1 hidden layer met een beperkt aantal neuronen. Evenveel neuronen als dimensies van je embedding. De output is niet perse wat je zoekt. Die embedding zit in je hidden layer."
   ],
   "id": "b6d441a9a1fefd7f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
